{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User routes on the site\n",
    "## Description\n",
    "**Clickstream** is a sequence of user actions on a website. It allows you to understand how users interact with the site. In this task, you need to find the most frequent custom routes.\n",
    "\n",
    "## Input data\n",
    "Input data is а table with clickstream data in file `hdfs:/data/clickstream.csv`.\n",
    "\n",
    "### Table structure\n",
    "* `user_id (int)` - Unique user identifier.\n",
    "* `session_id (int)` - Unique identifier for the user session. The user's session lasts until the identifier changes.\n",
    "* `event_type (string)` - Event type from the list:\n",
    "    * **page** - visit to the page\n",
    "    * **event** - any action on the page\n",
    "    * <b>&lt;custom&gt;</b> - string with any other type\n",
    "* `event_type (string)` - Page on the site.\n",
    "* `timestamp (int)` - Unix-timestamp of action.\n",
    "\n",
    "### Browser errors\n",
    "Errors can sometimes occur in the user's browser - after such an error appears, we can no longer trust the data of this session and all the following lines after the error or at the same time with it are considered corrupted and **should not be counted** in statistics.\n",
    "\n",
    "When an error occurs on the page, a random string containing the word **error** will be written to the `event_type` field.\n",
    "\n",
    "### Sample of user session\n",
    "<pre>\n",
    "+-------+----------+------------+----------+----------+\n",
    "|user_id|session_id|  event_type|event_page| timestamp|\n",
    "+-------+----------+------------+----------+----------+\n",
    "|    562|       507|        page|      main|1620494781|\n",
    "|    562|       507|       event|      main|1620494788|\n",
    "|    562|       507|       event|      main|1620494798|\n",
    "|    562|       507|        page|    family|1620494820|\n",
    "|    562|       507|       event|    family|1620494828|\n",
    "|    562|       507|        page|      main|1620494848|\n",
    "|    562|       507|wNaxLlerrorU|      main|1620494865|\n",
    "|    562|       507|       event|      main|1620494873|\n",
    "|    562|       507|        page|      news|1620494875|\n",
    "|    562|       507|        page|   tariffs|1620494876|\n",
    "|    562|       507|       event|   tariffs|1620494884|\n",
    "|    562|       514|        page|      main|1620728918|\n",
    "|    562|       514|       event|      main|1620729174|\n",
    "|    562|       514|        page|   archive|1620729674|\n",
    "|    562|       514|        page|     bonus|1620729797|\n",
    "|    562|       514|        page|   tariffs|1620731090|\n",
    "|    562|       514|       event|   tariffs|1620731187|\n",
    "+-------+----------+------------+----------+----------+\n",
    "</pre>\n",
    "\n",
    "#### Correct user routes for a given user:\n",
    "* **Session 507**: main-family-main\n",
    "* **Session 514**: main-archive-bonus-tariffs\n",
    "\n",
    "Route elements are ordered by the time they appear in the clickstream, from earliest to latest.\n",
    "\n",
    "The route must be accounted for completely before the end of the session or an error in the session.\n",
    "\n",
    "## Task\n",
    "You need to use the Spark SQL, Spark RDD and Spark DF interfaces to create a solution file, the lines of which contain **the 30 most frequent user routes** on the site.\n",
    "\n",
    "Each line of the file should contain the `route` and `count` values **separated by tabs**, where:\n",
    "* `route` - route on the site, consisting of pages separated by \"-\".\n",
    "* `count` - the number of user sessions in which this route was.\n",
    "\n",
    "The lines must be **ordered in descending order** of the `count` field.\n",
    "\n",
    "## Criteria\n",
    "You can get maximum of 3.5 points (final grade) for this assignment, depedning on the number of interface you manage to leverage. The criteria are as follows:\n",
    "\n",
    "* 0.5 points – Spark SQL solution with 1 query\n",
    "* 0.5 points – Spark SQL solution with <=2 queries\n",
    "* 0.5 points – Spark RDD solution\n",
    "* 0.5 points – Spark DF solution\n",
    "* 0.5 points – your solution algorithm is relatively optimized, i.e.: no O^2 or O^3 complexities; appropriate object usage; no data leaks etc. This is evaluated by staff.\n",
    "* 1 point – 1 on 1 screening session. During this session staff member can ask you questions regarding your solution logic, framework usage, questionable parts of your code etc. If your code is clean enough, the staff member can just ask you to solve a theoretical problem connected to Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-10-10 16:11:19,529 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='jupyter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `clickstream.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -copyFromLocal clickstream.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = se.read.csv(\"clickstream.csv\", header=True, inferSchema=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- session_id: integer (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- event_page: string (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+----------+----------+\n",
      "|user_id|session_id|  event_type|event_page| timestamp|\n",
      "+-------+----------+------------+----------+----------+\n",
      "|    562|       507|        page|      main|1695584127|\n",
      "|    562|       507|       event|      main|1695584134|\n",
      "|    562|       507|       event|      main|1695584144|\n",
      "|    562|       507|       event|      main|1695584147|\n",
      "|    562|       507|wNaxLlerrorU|      main|1695584154|\n",
      "|    562|       507|       event|      main|1695584154|\n",
      "|    562|       507|       event|      main|1695584154|\n",
      "|    562|       507|       event|      main|1695584160|\n",
      "|    562|       507|        page|    rabota|1695584166|\n",
      "|    562|       507|       event|    rabota|1695584174|\n",
      "|    562|       507|       event|    rabota|1695584181|\n",
      "|    562|       507|       event|    rabota|1695584189|\n",
      "|    562|       507|        page|      main|1695584194|\n",
      "|    562|       507|       event|      main|1695584204|\n",
      "|    562|       507|       event|      main|1695584211|\n",
      "|    562|       507|       event|      main|1695584211|\n",
      "|    562|       507|       event|      main|1695584219|\n",
      "|    562|       507|        page|     bonus|1695584221|\n",
      "|    562|       507|        page|    online|1695584222|\n",
      "|    562|       507|       event|    online|1695584230|\n",
      "+-------+----------+------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable(\"clicks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. SQL Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **STEP 1**:\n",
    "    * Use the LAG function to create a derived table (`DERIVED TABLE 1`) containing a column (`lag_event`) with timestampes lagged by 1 position\n",
    "    * Self-join the derived table with the original `clicks` table\n",
    "\n",
    "\n",
    "* **STEP 2**: Derive a table (`DERIVED TABLE 2`) by extracting rows where the `event_type` column contains an error event (e.g. `wNaxLlerrorU`)\n",
    "\n",
    "\n",
    "* **STEP 3**: Join the DERIVED TABLE 1 and the DERIVED TABLE 2 so that any timestamp occurring after error is tagged with `1` (1 = `error`) within both the same `user-id` and the same `session-id`\n",
    "\n",
    "\n",
    "* **STEP 4**: Derive a final table (`FINAL TABLE`) by filtering out errors and rows where the current event is the same as the previous event.\n",
    "\n",
    "\n",
    "* **STEP 5**: Take the FINAL TABLE to construct a route list for each `user/session` combination \n",
    "\n",
    "\n",
    "* **STEP 6**: Count the resulting routes, sort them in descending order and show top 30 routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 446:==================================================>    (30 + 2) / 33]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-----------+\n",
      "|route                    |route_count|\n",
      "+-------------------------+-----------+\n",
      "|main                     |8185       |\n",
      "|archive - main           |1110       |\n",
      "|main - rabota            |1045       |\n",
      "|internet - main          |895        |\n",
      "|bonus - main             |870        |\n",
      "|main - news              |768        |\n",
      "|main - tariffs           |677        |\n",
      "|main - online            |587        |\n",
      "|main - vklad             |517        |\n",
      "|archive - main - rabota  |336        |\n",
      "|bonus - main - rabota    |271        |\n",
      "|main - news - rabota     |263        |\n",
      "|archive - bonus - main   |260        |\n",
      "|archive - internet - main|253        |\n",
      "|internet - main - rabota |251        |\n",
      "|archive - main - news    |238        |\n",
      "|internet - main - news   |211        |\n",
      "|archive - main - tariffs |206        |\n",
      "|bonus - internet - main  |206        |\n",
      "|internet - main - tariffs|202        |\n",
      "|bonus - main - news      |169        |\n",
      "|archive - main - online  |163        |\n",
      "|main - rabota - tariffs  |158        |\n",
      "|bonus - main - tariffs   |158        |\n",
      "|main - online - rabota   |157        |\n",
      "|main - rabota - vklad    |147        |\n",
      "|internet - main - online |130        |\n",
      "|bonus - main - online    |130        |\n",
      "|archive - main - vklad   |125        |\n",
      "|main - news - tariffs    |121        |\n",
      "+-------------------------+-----------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "se.sql(\"\"\"\n",
    "SELECT -- STEP 6: COUNT the resulting routes, sort them in descending order and show top 30 routes\n",
    "    route, \n",
    "    COUNT(*) AS route_count\n",
    "FROM (\n",
    "    SELECT  -- STEP 5: Take the FINAL TABLE to construct a route list for each `user/session` combination \n",
    "        user_id, \n",
    "        session_id, \n",
    "        concat_ws(' - ', sort_array(collect_list(struct(event_page))).event_page) as route\n",
    "    FROM (\n",
    "        SELECT \n",
    "            user_id,\n",
    "            session_id,\n",
    "            event_page\n",
    "        FROM (\n",
    "            SELECT \n",
    "                cl.user_id,\n",
    "                cl.session_id,\n",
    "                cl.event_type,\n",
    "                cl.event_page,\n",
    "                cl.timestamp,\n",
    "                cl.timestamp_lag,\n",
    "                cl.lag_event,\n",
    "                err.error_event\n",
    "            FROM (          -- STEP 1: CREATE A DERIVED TABLE: This table will store lagged events\n",
    "                SELECT      -- We'll use the derived table to remove events ocurring recurrently on the same page\n",
    "                    cl1.user_id,\n",
    "                    cl1.session_id,\n",
    "                    cl1.event_type,\n",
    "                    cl1.event_page,\n",
    "                    cl1.timestamp,\n",
    "                    cl1.timestamp_lag,\n",
    "                    cl2.event_page AS lag_event\n",
    "                FROM (\n",
    "                    SELECT\n",
    "                        *, -- HERE WE OBTAIN VALUES LAGGED BY ONE POSITION FOR THE TIMESTAMP COLUMN\n",
    "                        LAG(cl.timestamp, 1, 0) OVER (PARTITION BY cl.user_id, cl.session_id ORDER BY cl.timestamp ASC) AS timestamp_lag\n",
    "                    FROM clicks AS cl\n",
    "                ) AS cl1\n",
    "                LEFT JOIN \n",
    "                    clicks AS cl2\n",
    "                ON \n",
    "                    cl1.user_id = cl2.user_id\n",
    "                    AND cl1.session_id = cl2.session_id\n",
    "                    AND cl1.timestamp_lag = cl2.timestamp\n",
    "                ORDER BY cl1.user_id, cl1.session_id, cl1.timestamp\n",
    "            ) AS cl\n",
    "            LEFT JOIN ( \n",
    "                SELECT user_id, session_id, timestamp, 1 AS error_event -- STEP 2: DERIVE A TABLE CONTAINING ONLY ROWS WITH USER SESSION ERRORS\n",
    "                FROM clicks                                                  -- MARK ERRORS WITH 1 IN A SEPARATE COLUMN\n",
    "                WHERE event_type LIKE \"%error%\"\n",
    "            ) AS err\n",
    "            ON -- STEP 3: JOIN THE TABLES FROM STEP 1 and STEP 2 SO THAT ALL ROWS WITH TIMESTAMPS ON or AFTER SESSION ERROR ARE MARKED WITH 1 AS ERRORS\n",
    "                cl.user_id = err.user_id\n",
    "                AND cl.session_id = err.session_id\n",
    "                AND cl.timestamp >= err.timestamp\n",
    "            ORDER BY cl.user_id, cl.session_id, cl.timestamp) AS tab1\n",
    "        WHERE  -- STEP 4: DERIVE A FINAL TABLE BY FILTERING OUT ERRORS AND ROWS WHERE A CURRENT EVENT IS THE SAME AS THE PREVIOUS ONE\n",
    "            error_event IS NULL \n",
    "            AND NOT event_page = lag_event\n",
    "            OR lag_event IS NULL) AS tbl1\n",
    "    GROUP BY user_id, session_id\n",
    "    ORDER BY user_id, session_id) AS tbl2\n",
    "GROUP BY route\n",
    "ORDER BY route_count DESC;\n",
    "\"\"\").show(30, False)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
